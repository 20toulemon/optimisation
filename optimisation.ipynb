{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd08dbf5bc89dab742f5de30b23787f0226eaae1944205194a1ea1a76bd94eff922",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "L'enclos à mouton"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "1. Etude du problème d'optimisation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Question 1:\n",
    "    La variable à optimiser est la variable $z = (y_0,...,y_i,...,y_N)$. Cette variable est de taille $n = N+1$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Qestion 2:\n",
    "    La coût a optimiser est $$J(z) = -\\sum_{i=0}^{N-1} \\frac{y_{i+1}+y_i}{2}d_i$$ Soit l'opposée de l'aire comprise entre la bergerie et la clôture."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Question 3:\n",
    "Les contraintes égalités associés au problème sont $$g(z) = \\sum^{N-1}_{i=0} \\sqrt{(d_i)^2+(y_{i+1}-y_i)^2} - L$$\n",
    "On veut en effet que la longeur de la clôture soit égale à L\n",
    "On peut alors exprimer le problème sous la forme:\n",
    "$$\\min_{z \\in \\mathbb{R}^n, g(z)=0} J(z)$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Question 4:\n",
    "A priori le problème est convexe en effet la fonction J est convexe, de même que l'ensemble de définition.\n",
    "On a en effet :\n",
    "$$\\forall \\lambda \\in [0,1], x \\in \\mathbb{R}^n, y \\in \\mathbb{R}^n, J(\\lambda x +(1-\\lambda)y) = \\lambda J(x)+(1-\\lambda) J(y)$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "2. Résolution numérique"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Question 5:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import casadi\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is Ipopt version 3.12.3, running with linear solver mumps.\nNOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n\nNumber of nonzeros in equality constraint Jacobian...:        0\nNumber of nonzeros in inequality constraint Jacobian.:       43\nNumber of nonzeros in Lagrangian Hessian.............:       81\n\nTotal number of variables............................:       41\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        0\nTotal number of inequality constraints...............:        3\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        3\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0 -2.4375000e+001 0.00e+000 4.04e-001  -1.0 0.00e+000    -  0.00e+000 0.00e+000   0\n   1 -5.1131329e+001 3.24e+000 3.42e+000  -1.7 2.01e+001    -  1.00e+000 2.04e-001f  1\n   2 -3.8861037e+001 1.59e+000 9.53e-001  -1.7 2.55e+000    -  1.00e+000 1.00e+000h  1\n   3 -4.0264095e+001 9.50e-001 2.18e-001  -1.7 1.03e+000    -  1.00e+000 1.00e+000f  1\n   4 -3.5715756e+001 1.54e-002 3.24e-002  -1.7 6.86e-001    -  1.00e+000 1.00e+000h  1\n   5 -3.5672233e+001 0.00e+000 6.48e-004  -2.5 1.82e-002    -  1.00e+000 1.00e+000h  1\n   6 -3.5679443e+001 0.00e+000 3.11e-007  -3.8 7.99e-004    -  1.00e+000 1.00e+000h  1\n   7 -3.5679889e+001 0.00e+000 2.71e-011  -5.7 4.57e-005    -  1.00e+000 1.00e+000f  1\n   8 -3.5679894e+001 0.00e+000 2.49e-014  -8.6 5.67e-007    -  1.00e+000 1.00e+000f  1\n\nNumber of Iterations....: 8\n\n                                   (scaled)                 (unscaled)\nObjective...............: -3.5679894234787092e+001  -3.5679894234787092e+001\nDual infeasibility......:  2.4868995751603507e-014   2.4868995751603507e-014\nConstraint violation....:  0.0000000000000000e+000   0.0000000000000000e+000\nComplementarity.........:  2.5059128801513861e-009   2.5059128801513861e-009\nOverall NLP error.......:  2.5059128801513861e-009   2.5059128801513861e-009\n\n\nNumber of objective function evaluations             = 9\nNumber of objective gradient evaluations             = 9\nNumber of equality constraint evaluations            = 0\nNumber of inequality constraint evaluations          = 9\nNumber of equality constraint Jacobian evaluations   = 0\nNumber of inequality constraint Jacobian evaluations = 9\nNumber of Lagrangian Hessian evaluations             = 8\nTotal CPU secs in IPOPT (w/o function evaluations)   =      0.041\nTotal CPU secs in NLP function evaluations           =      0.003\n\nEXIT: Optimal Solution Found.\n      solver  :   t_proc      (avg)   t_wall      (avg)    n_eval\n       nlp_f  |        0 (       0)        0 (       0)         9\n       nlp_g  |   1.00ms (111.11us)   1.00ms (111.67us)         9\n  nlp_grad_f  |        0 (       0)        0 (       0)        10\n  nlp_hess_l  |   2.00ms (250.00us)   2.01ms (250.75us)         8\n   nlp_jac_g  |        0 (       0)        0 (       0)        10\n       total  |  49.00ms ( 49.00ms)  48.72ms ( 48.72ms)         1\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([0,\n",
       "  0.25,\n",
       "  0.5,\n",
       "  0.75,\n",
       "  1.0,\n",
       "  1.25,\n",
       "  1.5,\n",
       "  1.75,\n",
       "  2.0,\n",
       "  2.25,\n",
       "  2.5,\n",
       "  2.75,\n",
       "  3.0,\n",
       "  3.25,\n",
       "  3.5,\n",
       "  3.75,\n",
       "  4.0,\n",
       "  4.25,\n",
       "  4.5,\n",
       "  4.75,\n",
       "  5.0,\n",
       "  5.25,\n",
       "  5.5,\n",
       "  5.75,\n",
       "  6.0,\n",
       "  6.25,\n",
       "  6.5,\n",
       "  6.75,\n",
       "  7.0,\n",
       "  7.25,\n",
       "  7.5,\n",
       "  7.75,\n",
       "  8.0,\n",
       "  8.25,\n",
       "  8.5,\n",
       "  8.75,\n",
       "  9.0,\n",
       "  9.25,\n",
       "  9.5,\n",
       "  9.75,\n",
       "  10.0],\n",
       " array([1.00094988e-05, 1.19009184e+00, 1.81497694e+00, 2.27415659e+00,\n",
       "        2.64343271e+00, 2.95290814e+00, 3.21815037e+00, 3.44844123e+00,\n",
       "        3.64983206e+00, 3.82651522e+00, 3.98152345e+00, 4.11712042e+00,\n",
       "        4.23503411e+00, 4.33660380e+00, 4.42287656e+00, 4.49467249e+00,\n",
       "        4.55262984e+00, 4.59723671e+00, 4.62885320e+00, 4.64772676e+00,\n",
       "        4.65400206e+00, 4.64772676e+00, 4.62885320e+00, 4.59723671e+00,\n",
       "        4.55262984e+00, 4.49467249e+00, 4.42287656e+00, 4.33660380e+00,\n",
       "        4.23503411e+00, 4.11712042e+00, 3.98152345e+00, 3.82651522e+00,\n",
       "        3.64983206e+00, 3.44844123e+00, 3.21815037e+00, 2.95290814e+00,\n",
       "        2.64343271e+00, 2.27415659e+00, 1.81497694e+00, 1.19009184e+00,\n",
       "        1.00094988e-05]))"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "def optimal_curve(a, b, L, N = 40, d = None, init_guess = None):\n",
    "    if L < b-a:\n",
    "        return \"Le problème ne peut pas être résolu\"\n",
    "\n",
    "    if d == None:\n",
    "        d = [(b-a)/N]*N\n",
    "    \n",
    "    x_opt = [a]\n",
    "    for i in range(N):\n",
    "        x_opt.append(x_opt[i]+d[i])\n",
    "    \n",
    "    if init_guess == None:\n",
    "        M = L-(b-a)\n",
    "        debut=[0]\n",
    "        for i in range(N-1):\n",
    "            debut.append(M/2)\n",
    "        debut.append(0)\n",
    "        init_guess = np.array(debut)\n",
    "    \n",
    "    opti = casadi.Opti()\n",
    "    y = opti.variable(N+1)\n",
    "    J = 0\n",
    "    for i in range(N):\n",
    "        J -= (y[i+1]+y[i])/2*(x_opt[i+1]-x_opt[i])\n",
    "    opti.minimize(J)\n",
    "    f = [-L]\n",
    "    for i in range(N):\n",
    "        f.append(np.sqrt((x_opt[i+1]-x_opt[i])**2+(y[i+1]-y[i])**2))\n",
    "    opti.subject_to(np.sum(f) <= 10**-5)\n",
    "    opti.subject_to(y[0] <= 10**-5)\n",
    "    opti.subject_to(y[N] <= 10**-5)\n",
    "    opti.set_initial(y, init_guess)\n",
    "    opti.solver('ipopt')\n",
    "    yobt = opti.solve()\n",
    "    y_opt = yobt.value(y)\n",
    "    return x_opt, y_opt\n",
    "optimal_curve(0, 10, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}